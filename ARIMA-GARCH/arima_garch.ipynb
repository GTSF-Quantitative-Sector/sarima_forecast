{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:09:05.327889Z",
     "start_time": "2025-02-20T19:09:03.538041Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import detrend, find_peaks, stft, periodogram\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "from statsmodels.tsa.filters.hp_filter import hpfilter\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d6c37c66707c31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:09:13.010743Z",
     "start_time": "2025-02-20T19:09:09.276856Z"
    }
   },
   "outputs": [],
   "source": [
    "def advanced_detrend(df, column=\"Close\", lamb=14400):\n",
    "    \"\"\"\n",
    "    Uses the Hodrick-Prescott filter to extract the trend.\n",
    "    \"\"\"\n",
    "    cycle, trend = hpfilter(df[column], lamb=lamb)\n",
    "    df[\"Trend\"] = trend\n",
    "    df[\"Cycle\"] = cycle\n",
    "    return df, trend\n",
    "\n",
    "def preprocess_data(df, column=\"Open\"):\n",
    "    \"\"\"\n",
    "    A simple detrending of a series using scipy's detrend.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[column] = detrend(df[column])\n",
    "    return df\n",
    "\n",
    "def check_stationarity(series, significance=0.05):\n",
    "    \"\"\"\n",
    "    Performs the Augmented Dickey-Fuller (ADF) test on the series. (Either will use Arma or arima based off stationarity)\n",
    "    Prints the test statistic and p-value.\n",
    "    \"\"\"\n",
    "    series = series.dropna()\n",
    "    result = adfuller(series)\n",
    "    print(\"ADF Statistic: {:.4f}\".format(result[0]))\n",
    "    print(\"p-value: {:.4f}\".format(result[1]))\n",
    "    if result[1] < significance:\n",
    "        print(\"The series is stationary.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The series is non-stationary.\")\n",
    "        return False\n",
    "\n",
    "def fourier_analysis(df, dataset, column=\"Trend\"):\n",
    "    \"\"\"\n",
    "    Uses Fourier transform to identify dominant frequencies (seasonality) in the data.\n",
    "    \"\"\"\n",
    "    N = len(df)\n",
    "    T = 1\n",
    "    y = df[column].values\n",
    "    yf = fft(y)\n",
    "    xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "    amplitudes = 2.0 / N * np.abs(yf[:N//2])\n",
    "    peaks, _ = find_peaks(amplitudes, height=0.01 * max(amplitudes))\n",
    "    print(\"Detected peaks:\", peaks[:5])\n",
    "    sorted_indices = np.argsort(amplitudes[peaks])[::-1]  # sort descending\n",
    "    strongest_peaks = peaks[sorted_indices][:5]\n",
    "\n",
    "    dominant_periods = 1 / xf[strongest_peaks]\n",
    "    dominant_amplitudes = amplitudes[strongest_peaks]\n",
    "    print(\"Strongest detected seasonal periods (in days) and their amplitudes:\")\n",
    "    for period, amplitude in zip(dominant_periods, dominant_amplitudes):\n",
    "        print(f\"Period: {period:.2f} days, Amplitude: {amplitude:.5f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(xf, amplitudes)\n",
    "    plt.xlabel(\"Frequency (1/day)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(f\"Fourier Transform - Seasonality Detection ({dataset})\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return dominant_periods, dominant_amplitudes\n",
    "def plot_hp_filter_results(df, dataset_name, column=\"Close\", lamb=14400): \n",
    "    \"\"\"\n",
    "    Function used to plot the different hp filter trends, cycles and original data\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df.index, df[column], label=\"Original Close Price\", linewidth=1)\n",
    "    plt.plot(df.index, df[\"Trend\"], label=f\"HP Trend (lambda={lamb})\", linewidth=2, color='red')\n",
    "    plt.plot(df.index, df[\"Cycle\"], label=f\"HP Cycle (lambda={lamb})\", linewidth=1, linestyle='--', color='green', alpha=0.7)\n",
    "    plt.title(f\"Hodrick-Prescott Filter Decomposition for {dataset_name} (Lambda = {lamb})\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "datasets = [\"../data/CORN-Prices.csv\", \"../data/GLD-Prices.csv\", \"../data/SLV-Prices.csv\",\"../data/WEAT-Prices.csv\"]\n",
    "lambda_values = [100, 1600, 14400, 129600] # Test lambda values\n",
    "\n",
    "for dataset in datasets:\n",
    "    start, end = \"/\", \"-\"\n",
    "    commodity_name = dataset.split(\"/\")[-1].split(\".\")[0]\n",
    "    #print(f\"\\n--- Processing dataset: {commodity_name} ---\")\n",
    "    if not os.path.exists(dataset):\n",
    "        print(f\"Error: Dataset file not found: {dataset}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(dataset, parse_dates=[\"Date\"])\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "\n",
    "    # for lamb in lambda_values: # looping through different lambda values\n",
    "    #     print(f\"\\n--- Lambda: {lamb} ---\")\n",
    "    #     df_processed, cycle = advanced_detrend(df.copy(), lamb=lamb)  # HP Filter\n",
    "    #     print(df_processed.head())\n",
    "    #     #plot_hp_filter_results(df_processed, commodity_name, lamb=lamb) # Plot results for each lambda\n",
    "\n",
    "    # fourier_analysis(df_processed, commodity_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c3f27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sarima_garch(train_data, seasonal_periods, garch_order=(1, 1)):\n",
    "    \"\"\"\n",
    "    Uses auto arima to fit sarima model with seasonal comps and then GARCH model for dealing with residuals.\n",
    "    \"\"\"\n",
    "    # Fit SARIMA model via auto_arima\n",
    "    model = pm.auto_arima(\n",
    "        train_data,\n",
    "        seasonal=True,\n",
    "        m=seasonal_periods,\n",
    "        stepwise=True,\n",
    "        suppress_warnings=True,\n",
    "        trace=True\n",
    "    )\n",
    "    sarima_order = model.order\n",
    "    sarima_seasonal_order = model.seasonal_order\n",
    "    print(f\"Fitted SARIMA order: {sarima_order}, seasonal order: {sarima_seasonal_order}\")\n",
    "\n",
    "    # Fit SARIMA using SARIMAX\n",
    "    sarima_model = SARIMAX(train_data, order=sarima_order, seasonal_order=sarima_seasonal_order)\n",
    "    sarima_results = sarima_model.fit(disp=0)\n",
    "\n",
    "    residuals = sarima_results.resid.dropna()\n",
    "    garch = arch_model(residuals, vol='Garch', p=garch_order[0], q=garch_order[1])\n",
    "    garch_fit = garch.fit(update_freq=5, disp='off')\n",
    "\n",
    "    return sarima_results, garch_fit\n",
    "\n",
    "def forecast_sarima_garch(sarima_model, garch_model, steps):\n",
    "    \"\"\"\n",
    "    Predicts future values using SARIMA GARCH model\n",
    "    \"\"\"\n",
    "    sarima_forecast = sarima_model.get_forecast(steps=steps)\n",
    "    mean_forecast = sarima_forecast.predicted_mean\n",
    "\n",
    "    garch_forecast = garch_model.forecast(horizon=steps)\n",
    "    volatility = np.sqrt(garch_forecast.variance.iloc[-1])\n",
    "\n",
    "    return mean_forecast, volatility\n",
    "\n",
    "def analyze_commodity(dataset_path, forecast_steps=30):\n",
    "    \"\"\"\n",
    "    Loads the commodity data, applies detrending, checks for stationarity using the ADF test,\n",
    "    Runs FFT, fits SARIMA-GARCH model, and forecasts future prices.\n",
    "    \"\"\"\n",
    "    commodity_name = dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(dataset_path, parse_dates=[\"Date\"])\n",
    "    df = df[df[\"Date\"] >= pd.to_datetime(\"2020-01-01\")] # Filtering only data in last 5 years\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "    # HF Filter\n",
    "    df_processed, trend = advanced_detrend(df.copy())\n",
    "    data = df_processed[\"Trend\"]\n",
    "\n",
    "    print(f\"Performing ADF Test on the original {commodity_name} series:\")\n",
    "    is_stationary = check_stationarity(trend)\n",
    "    if not is_stationary:\n",
    "        print(\"Data is non-stationary. auto_arima will apply differencing as needed to achieve stationarity.\\n\")\n",
    "    else:\n",
    "        print(\"Data is already stationary.\\n\")\n",
    "\n",
    "    # Training and testing splits\n",
    "    train_size = int(len(df_processed) * 0.8)\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "\n",
    "    #running fourier on column \"Trend\"\n",
    "    dominant_periods, top_amplitudes = fourier_analysis(df_processed, commodity_name) \n",
    "\n",
    "    if dominant_periods is None or []:\n",
    "        # Find best seasonal period using periodogram\n",
    "        freqs, pxx = periodogram(train, detrend='linear')\n",
    "        dominant_index = np.argmax(pxx[1:]) + 1  # ignore zero frequency\n",
    "        dominant_freq = freqs[dominant_index]\n",
    "        dominant_period = int(1 / dominant_freq)\n",
    "        print(f\"Using dominant seasonal period determined from periodogram: {dominant_period} days\\n\")\n",
    "    else:\n",
    "        dominant_period = int(round(dominant_periods[0]))\n",
    "        print(f\"Using dominant seasonal period from Fourier analysis: {dominant_period} days\\n\")\n",
    "\n",
    "    sarima_model, garch_model = fit_sarima_garch(train, seasonal_periods=dominant_period)\n",
    "    print(\"Finished fitting SARIMA-GARCH models.\\n\")\n",
    "\n",
    "    mean_forecast, volatility = forecast_sarima_garch(sarima_model, garch_model, steps=forecast_steps)\n",
    "\n",
    "    forecast_index = pd.date_range(start=train.index[-1], periods=forecast_steps + 1, freq='D')[1:]\n",
    "    predictions = pd.DataFrame({\n",
    "        'Mean': mean_forecast.values,\n",
    "        'Lower': mean_forecast - 1.96 * volatility,\n",
    "        'Upper': mean_forecast + 1.96 * volatility\n",
    "    }, index=forecast_index)\n",
    "\n",
    "    # Plot forecasts vs actual data\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(train.index, train, label='Training Data')\n",
    "    plt.plot(test.index, test, label='Actual Values')\n",
    "    plt.plot(predictions.index, predictions['Mean'], label='SARIMA-GARCH Forecast')\n",
    "    plt.fill_between(predictions.index, predictions['Lower'], predictions['Upper'], color='gray', alpha=0.2)\n",
    "    plt.title(f'{commodity_name} Price Forecast with Volatility Bands')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # GARCH diagnostics plot\n",
    "    garch_model.plot(annualize='D')\n",
    "    plt.suptitle(f'GARCH Model Diagnostics - {commodity_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a295612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"../data/CORN-Prices.csv\", \"../data/GLD-Prices.csv\", \"../data/SLV-Prices.csv\",\"../data/WEAT-Prices.csv\"]\n",
    "for dataset in datasets:\n",
    "    analyze_commodity(dataset, forecast_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328ac87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commodities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
